
## 内容

该模式下，用户的所有交互都会触发预设的流程
## 应用场景

>（适用于技能和处理逻辑相对固定、可预测的场景）
- 售后客服中的标准问答流程
- 根据用户请求逐段生成长文本内容
- 
---
## 收费

- 调试时，消息计费按智能体在 **Single Agent (LLM)** 模式下所选模型计费（默认为 **GPT-4o 8k**）。
- 发布后，无论 Chatflow 中是否包含 LLM 节点，均统一按 **GPT-4o 8k** 计费
（一刀切是否使用，皆以**GPT-4o 8k** 计费）
## 操作流程

1. 在构建页面，将模式从 **Single Agent (LLM Mode)** 切换为 **Single Agent (Chatflow Mode)**
2. **配置 Start 节点**
>💡 注意：若使用自定义变量，需通过 API 或 Chat SDK 发布调试，Coze 平台界面调试会报错。
>如需在平台预览，可在 Memory > Variables 中配置。
3. 拖拽并连接其他节点（如 LLM、知识库、条件判断、插件等
4. 按提示将 Chatflow 绑定到当前智能体
5. 配置智能体设置
6. 调试与发布
## 限制说明

>每个智能体智能体只能配置一个chatflow
>不能直接设置插件、提示词或知识库，但可以在内部设置节点引用
>不支持在 Chatflow 模式下提供“用户提问建议”


---
## 疑问阶段
- 传统的LLM模式是什么？
	- 就是prompt＋插件调用的模式
	- 当下痛点：**时常不可控**+**逻辑难保证**
 - 具体怎么收费
	 - 一刀切
 - 和工作流的区别是什么
	 - prompt+工作流模式下：调用权仍在ai手中
	 - （你雇了个聪明员工，告诉他：“遇到技术问题就去查手册（工作流）”，但他可能偷懒不查。）
 - 不支持在 Chatflow 模式下提供“用户提问建议”
	 - 预设问题和引导提问已经上线
	 - 在智能体回复后，自动根据对话内容提供 3 条用户提问建议
 -  API 或 Chat SDK 发布调试
	 - API发布:提供URL+密钥（python/JavaScript）
	 - Chat SDK发布：**嵌入代码**（JavaScript）
	 - 预设版只有用户输入，而在C/A可以传入自定义变量
